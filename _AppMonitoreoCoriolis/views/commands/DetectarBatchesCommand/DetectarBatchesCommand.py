import logging
import hashlib
import pytz
from datetime import datetime
from django.utils import timezone as django_timezone
from django.db import IntegrityError
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework.permissions import IsAuthenticated
from _AppComplementos.models import Sistema, ConfiguracionCoeficientes
from _AppMonitoreoCoriolis.models import NodeRedData, BatchDetectado
from UTIL_LIB.conversiones import lb_s_a_kg_min, lb_a_kg, cm3_a_gal
from _AppMonitoreoCoriolis.views.utils import COLOMBIA_TZ

# Configurar logging
logger = logging.getLogger(__name__)

class DetectarBatchesCommandView(APIView):
    """
    CBV para detectar batches en un rango de fechas espec√≠fico
    """
    permission_classes = [IsAuthenticated]
    
    def post(self, request, sistema_id):
        try:
            # Obtener par√°metros
            fecha_inicio_str = request.data.get('fecha_inicio')
            fecha_fin_str = request.data.get('fecha_fin')
            
            if not fecha_inicio_str or not fecha_fin_str:
                return Response({
                    'success': False,
                    'error': 'Las fechas de inicio y fin son obligatorias'
                }, status=400)
            
            # Verificar que el sistema existe
            sistema = Sistema.objects.get(id=sistema_id)
            
            # Obtener l√≠mites de configuraci√≥n (solo para lim_inf y lim_sup)
            try:
                config = ConfiguracionCoeficientes.objects.get(systemId=sistema)
                lim_inf = config.lim_inf_caudal_masico  # En kg/min
                lim_sup = config.lim_sup_caudal_masico  # En kg/min
                # YA NO usar vol_minimo ni time_finished_batch de configuraci√≥n
                # Estos valores ahora vienen din√°micamente de cada registro NodeRedData
            except ConfiguracionCoeficientes.DoesNotExist:
                return Response({
                    'success': False,
                    'error': 'No se encontr√≥ configuraci√≥n de l√≠mites para este sistema'
                }, status=400)
            
            # Parsear fechas con formato datetime (igual que las queries hist√≥ricas)
            try:
                # Intentar formato con fecha y hora: "2025-10-16T00:00:00"
                fecha_inicio_naive = datetime.strptime(fecha_inicio_str, '%Y-%m-%dT%H:%M:%S')
                fecha_fin_naive = datetime.strptime(fecha_fin_str, '%Y-%m-%dT%H:%M:%S')
            except ValueError:
                try:
                    # Fallback a formato solo fecha: "2025-10-16"
                    fecha_inicio_naive = datetime.strptime(fecha_inicio_str, '%Y-%m-%d')
                    fecha_fin_naive = datetime.strptime(fecha_fin_str, '%Y-%m-%d')
                    # Establecer horas para cubrir todo el rango del d√≠a
                    fecha_inicio_naive = fecha_inicio_naive.replace(hour=0, minute=0, second=0, microsecond=0)
                    fecha_fin_naive = fecha_fin_naive.replace(hour=23, minute=59, second=59, microsecond=999999)
                except ValueError:
                    return Response({
                        'success': False,
                        'error': 'Formato de fecha inv√°lido. Use YYYY-MM-DD o YYYY-MM-DDTHH:MM:SS'
                    }, status=400)
            
            # Asumir que las fechas del frontend est√°n en hora de Colombia y convertir a UTC
            fecha_inicio_colombia = COLOMBIA_TZ.localize(fecha_inicio_naive)
            fecha_fin_colombia = COLOMBIA_TZ.localize(fecha_fin_naive)
            
            # Convertir a UTC para consultas de base de datos (igual que otras queries)
            fecha_inicio = fecha_inicio_colombia.astimezone(pytz.UTC)
            fecha_fin = fecha_fin_colombia.astimezone(pytz.UTC)
            
            # Log para debugging de zona horaria
            logger.info(f"Fechas Colombia - Inicio: {fecha_inicio_colombia} | Fin: {fecha_fin_colombia}")
            logger.info(f"Fechas UTC (para DB) - Inicio: {fecha_inicio} | Fin: {fecha_fin}")
            logger.info(f"Input recibido - fecha_inicio_str: '{fecha_inicio_str}', fecha_fin_str: '{fecha_fin_str}'")
            
            # Obtener datos del rango de fechas, ordenados por fecha IoT
            datos = NodeRedData.objects.filter(
                systemId=sistema,
                created_at_iot__gte=fecha_inicio,
                created_at_iot__lte=fecha_fin,
                created_at_iot__isnull=False  # Solo datos con timestamp IoT v√°lido
            ).order_by('created_at_iot')
            
            if not datos.exists():
                return Response({
                    'success': False,
                    'error': 'No se encontraron datos en el rango de fechas especificado'
                }, status=404)
            
            # Ejecutar algoritmo de detecci√≥n de batches con perfil din√°mico
            batches_detectados = self._detectar_batches_con_perfil_dinamico(datos, lim_inf, lim_sup, sistema)
            
            # Guardar batches en la base de datos con prevenci√≥n de duplicados
            batches_guardados = []
            batches_existentes = 0
            
            for batch_data in batches_detectados:
                # Generar hash para este batch usando el perfil din√°mico que se us√≥
                hash_batch = self._generar_hash_batch(
                    batch_data['fecha_inicio'],
                    batch_data['fecha_fin'], 
                    sistema_id,
                    batch_data['vol_minimo_usado'],
                    batch_data['time_finished_usado']
                )
                
                try:
                    # Intentar crear el batch con el hash √∫nico
                    batch = BatchDetectado.objects.create(
                        systemId=sistema,
                        fecha_inicio=batch_data['fecha_inicio'],
                        fecha_fin=batch_data['fecha_fin'],
                        vol_total=batch_data['vol_total'],
                        mass_total=batch_data['mass_total'],
                        temperatura_coriolis_prom=batch_data['temperatura_coriolis_prom'],
                        densidad_prom=batch_data['densidad_prom'],
                        pressure_out_prom=batch_data.get('pressure_out_prom'),
                        hash_identificacion=hash_batch,
                        perfil_lim_inf_caudal=lim_inf,
                        perfil_lim_sup_caudal=lim_sup,
                        perfil_vol_minimo=batch_data['vol_minimo_usado'],
                        duracion_minutos=batch_data['duracion_minutos'],
                        total_registros=batch_data['total_registros'],
                        time_finished_batch=batch_data['time_finished_usado']
                    )
                except IntegrityError:
                    # El batch ya existe (por el hash √∫nico), buscar el existente
                    logger.info(f"Batch ya existe con hash {hash_batch[:16]}...")
                    batch = BatchDetectado.objects.get(hash_identificacion=hash_batch)
                    batches_existentes += 1
                batches_guardados.append({
                    'id': batch.id,
                    'fecha_inicio': batch.fecha_inicio.astimezone(COLOMBIA_TZ).strftime('%d/%m/%Y %H:%M:%S'),
                    'fecha_fin': batch.fecha_fin.astimezone(COLOMBIA_TZ).strftime('%d/%m/%Y %H:%M:%S'),
                    'vol_total': round(batch.vol_total, 2),
                    'mass_total': round(batch.mass_total, 2),
                    'temperatura_coriolis_prom': round(batch.temperatura_coriolis_prom, 2),
                    'densidad_prom': round(batch.densidad_prom, 10),
                    'pressure_out_prom': round(batch.pressure_out_prom, 2) if batch.pressure_out_prom else None,
                    'duracion_minutos': round(batch.duracion_minutos, 2),
                    'total_registros': batch.total_registros,
                    'perfil_lim_inf': batch.perfil_lim_inf_caudal,
                    'perfil_lim_sup': batch.perfil_lim_sup_caudal,
                    'perfil_vol_min': batch.perfil_vol_minimo
                })
            
            # Obtener TODOS los batches existentes en el rango de fechas (no solo los reci√©n detectados)
            todos_batches = BatchDetectado.objects.filter(
                systemId=sistema,
                fecha_inicio__gte=fecha_inicio,
                fecha_fin__lte=fecha_fin
            ).order_by('-fecha_inicio')
            
            batches_completos = []
            for batch in todos_batches:
                batches_completos.append({
                    'id': batch.id,
                    'fecha_inicio': batch.fecha_inicio.astimezone(COLOMBIA_TZ).strftime('%d/%m/%Y %H:%M:%S'),
                    'fecha_fin': batch.fecha_fin.astimezone(COLOMBIA_TZ).strftime('%d/%m/%Y %H:%M:%S'),
                    'vol_total': round(batch.vol_total, 2),
                    'mass_total': round(batch.mass_total, 2),
                    'temperatura_coriolis_prom': round(batch.temperatura_coriolis_prom, 2),
                    'densidad_prom': round(batch.densidad_prom, 10),
                    'pressure_out_prom': round(batch.pressure_out_prom, 2) if batch.pressure_out_prom else None,
                    'duracion_minutos': round(batch.duracion_minutos, 2),
                    'total_registros': batch.total_registros,
                    'perfil_lim_inf': batch.perfil_lim_inf_caudal or 0,
                    'perfil_lim_sup': batch.perfil_lim_sup_caudal or 0,
                    'perfil_vol_min': batch.perfil_vol_minimo or 0
                })
            
            return Response({
                'success': True,
                'batches_detectados': len(batches_completos),
                'batches_nuevos': len(batches_guardados) - batches_existentes,
                'batches_existentes': batches_existentes,
                'batches': batches_completos,
                'configuracion_usada': {
                    'lim_inf_caudal_masico': lim_inf,
                    'lim_sup_caudal_masico': lim_sup,
                    'nota': 'vol_detect_batch y time_closed_batch se usan din√°micamente de cada registro NodeRedData'
                },
                'rango_analizado': {
                    'fecha_inicio': fecha_inicio.astimezone(COLOMBIA_TZ).strftime('%d/%m/%Y %H:%M:%S'),
                    'fecha_fin': fecha_fin.astimezone(COLOMBIA_TZ).strftime('%d/%m/%Y %H:%M:%S'),
                    'total_registros': datos.count()
                }
            })
            
        except Sistema.DoesNotExist:
            return Response({
                'success': False,
                'error': 'Sistema no encontrado'
            }, status=404)
        except ValueError as e:
            return Response({
                'success': False,
                'error': f'Error en formato de fecha: {str(e)}'
            }, status=400)
        except Exception as e:
            logger.error(f"Error en DetectarBatchesCommandView: {str(e)}", exc_info=True)
            return Response({
                'success': False,
                'error': f'Error interno del servidor: {str(e)}'
            }, status=500)
    
    def _detectar_batches_con_perfil_dinamico(self, datos, lim_inf, lim_sup, sistema):
        """
        L√≥gica de detecci√≥n con PERFIL DIN√ÅMICO del PRIMER DATO:
        - Cada batch captura vol_detect_batch y time_closed_batch del PRIMER dato que lo inicia
        - El perfil se mantiene constante hasta que el batch se cierra
        - Detecta cuando caudal cambia de 0 a > 0 (inicio de batch)
        - Cuando caudal vuelve a 0, espera time_closed_batch minutos antes de cerrar
        - Si caudal sube antes del tiempo de espera, contin√∫a el batch
        - Compara masa total del √∫ltimo punto vs primer punto
        - Si la diferencia supera vol_detect_batch, es batch v√°lido
        
        Args:
            datos: QuerySet de NodeRedData ordenado por created_at_iot
            lim_inf: L√≠mite inferior de caudal m√°sico (kg/min) - SOLO PARA REFERENCIA
            lim_sup: L√≠mite superior de caudal m√°sico (kg/min) - SOLO PARA REFERENCIA
            sistema: Instancia del Sistema
        """
        batches = []
        en_batch = False
        inicio_batch = None
        primer_dato = None
        datos_batch = []
        punto_anterior = None
        tiempo_cero_inicio = None
        ultimo_dato_con_flujo = None
        
        # Perfil capturado del PRIMER dato del batch (se mantiene constante)
        vol_minimo_batch = None
        time_finished_batch_actual = None

        logger.info(f"üîç Iniciando detecci√≥n con PERFIL DIN√ÅMICO del primer dato de cada batch")

        for dato in datos:
            mass_rate_raw = dato.mass_rate  # En lb/sec
            total_mass = dato.total_mass
            total_volume = dato.total_volume
            timestamp_actual = dato.created_at_iot
            
            # LOG: Mostrar timestamp de cada registro para debugging
            logger.debug(f"üìÖ Procesando registro: {timestamp_actual} | mass_rate: {mass_rate_raw} lb/sec")
            
            # Verificar que tenemos los datos necesarios
            if mass_rate_raw is None or total_mass is None or total_volume is None:
                logger.debug(f"‚ö†Ô∏è Saltando registro con datos faltantes en {timestamp_actual}")
                continue
                
            # Convertir mass_rate de lb/sec a kg/min
            mass_rate_kg_min = lb_s_a_kg_min(mass_rate_raw)
            logger.debug(f"üîÑ Convertido: {mass_rate_kg_min:.3f} kg/min")
            
            # L√ìGICA DIN√ÅMICA: Detectar cambio de 0 a > 0 y manejar tiempo de espera con perfil del primer dato
            if mass_rate_kg_min > 0:
                if not en_batch:
                    # üéØ CAPTURAR PERFIL DEL PRIMER DATO
                    vol_detect = dato.vol_detect_batch
                    time_closed = dato.time_closed_batch
                    
                    # Validar que el dato tiene perfil v√°lido
                    if vol_detect is None or time_closed is None:
                        logger.warning(f"‚ö†Ô∏è Dato sin perfil v√°lido en {dato.created_at_local}: vol_detect={vol_detect}, time_closed={time_closed} - IGNORANDO")
                        continue
                    
                    # Capturar perfil para este batch
                    vol_minimo_batch = vol_detect
                    time_finished_batch_actual = time_closed
                    
                    # Iniciar nuevo batch - usar punto anterior como referencia inicial
                    en_batch = True
                    inicio_batch = dato.created_at_iot
                    tiempo_cero_inicio = None
                    
                    # Si tenemos punto anterior (donde flujo = 0), usarlo como referencia
                    if punto_anterior is not None:
                        primer_dato = punto_anterior
                        logger.info(f"‚úÖ Iniciando batch en {inicio_batch}, usando punto anterior como referencia")
                        logger.info(f"   Punto inicial (flujo=0): masa={primer_dato.total_mass} lb, volumen={primer_dato.total_volume} cm¬≥")
                        logger.info(f"   üéØ PERFIL CAPTURADO: vol_detect={vol_minimo_batch:.2f} kg, time_closed={time_finished_batch_actual:.2f} min")
                    else:
                        primer_dato = dato
                        logger.info(f"‚úÖ Iniciando batch en {inicio_batch}, sin punto anterior disponible")
                        logger.info(f"   üéØ PERFIL CAPTURADO: vol_detect={vol_minimo_batch:.2f} kg, time_closed={time_finished_batch_actual:.2f} min")
                    
                    datos_batch = [dato]
                    ultimo_dato_con_flujo = dato
                    logger.debug(f"Flujo actual: {mass_rate_kg_min:.2f} kg/min, masa actual: {total_mass} lb, volumen actual: {total_volume} cm¬≥")
                else:
                    # Continuar batch - el flujo volvi√≥ a subir, reiniciar contador de tiempo en cero
                    datos_batch.append(dato)
                    ultimo_dato_con_flujo = dato
                    
                    # Si hab√≠a un contador de tiempo en cero, reiniciarlo
                    if tiempo_cero_inicio is not None:
                        tiempo_antes_subir = timestamp_actual - tiempo_cero_inicio
                        minutos_antes_subir = tiempo_antes_subir.total_seconds() / 60
                        logger.info(f"üü¢ FLUJO VOLVI√ì A SUBIR - Continuando batch")
                        logger.info(f"‚è±Ô∏è Tiempo que estuvo en cero: {minutos_antes_subir:.3f} min (l√≠mite: {time_finished_batch_actual} min)")
                        logger.info(f"üîÑ Reiniciando contador de tiempo en cero")
                        tiempo_cero_inicio = None  # Reiniciar porque el flujo volvi√≥ a subir
                    else:
                        logger.debug(f"‚ûï Agregando datos al batch en curso (timestamp: {timestamp_actual})")
            else:
                # mass_rate <= 0: verificar tiempo de espera si estaba en batch
                if en_batch:
                    if tiempo_cero_inicio is None:
                        # Primera vez que el flujo cae a cero, iniciar contador
                        tiempo_cero_inicio = timestamp_actual
                        logger.info(f"üî¥ FLUJO CAY√ì A CERO - Iniciando contador de tiempo")
                        logger.info(f"‚è±Ô∏è Tiempo inicio cero: {tiempo_cero_inicio}")
                        logger.info(f"‚è≥ Esperando {time_finished_batch_actual} minutos para cerrar batch")
                    else:
                        # Verificar si ya pas√≥ el tiempo de espera
                        tiempo_transcurrido = timestamp_actual - tiempo_cero_inicio
                        minutos_en_cero = tiempo_transcurrido.total_seconds() / 60
                        
                        logger.debug(f"‚è∞ Registro actual: {timestamp_actual}")
                        logger.debug(f"üïê Tiempo desde inicio cero: {tiempo_transcurrido}")
                        logger.debug(f"üìä Minutos transcurridos: {minutos_en_cero:.3f} min / {time_finished_batch_actual} min")
                        
                        if minutos_en_cero >= time_finished_batch_actual:
                            # Ha pasado el tiempo de espera, cerrar el batch
                            fin_batch = tiempo_cero_inicio  # Usar el momento cuando empez√≥ a estar en cero
                            
                            logger.info(f"‚úÖ TIEMPO SUPERADO - Cerrando batch despu√©s de {minutos_en_cero:.3f} min en cero")
                            logger.info(f"üèÅ Batch se cierra en: {fin_batch}")
                            
                            # Calcular diferencias de masa y volumen entre √∫ltimo dato con flujo y primer punto
                            if primer_dato and ultimo_dato_con_flujo:
                                # C√°lculos de masa
                                masa_inicial_lb = primer_dato.total_mass
                                masa_final_lb = ultimo_dato_con_flujo.total_mass
                                diferencia_masa_lb = masa_final_lb - masa_inicial_lb
                                diferencia_masa_kg = lb_a_kg(diferencia_masa_lb)
                                
                                # C√°lculos de volumen (convertir de cm¬≥ a galones)
                                volumen_inicial_cm3 = primer_dato.total_volume
                                volumen_final_cm3 = ultimo_dato_con_flujo.total_volume
                                diferencia_volumen_cm3 = volumen_final_cm3 - volumen_inicial_cm3
                                diferencia_volumen_gal = cm3_a_gal(diferencia_volumen_cm3)
                                
                                logger.debug(f"Cerrando batch despu√©s de {minutos_en_cero:.2f} min en cero (l√≠mite: {time_finished_batch_actual} min)")
                                logger.debug(f"Masa inicial (punto en 0): {masa_inicial_lb:.2f} lb en {primer_dato.created_at_iot}")
                                logger.debug(f"Masa final (√∫ltimo punto >0): {masa_final_lb:.2f} lb en {ultimo_dato_con_flujo.created_at_iot}")
                                logger.debug(f"Diferencia masa: {diferencia_masa_lb:.2f} lb = {diferencia_masa_kg:.2f} kg")
                                logger.debug(f"Volumen inicial: {volumen_inicial_cm3:.2f} cm¬≥, Volumen final: {volumen_final_cm3:.2f} cm¬≥")
                                logger.debug(f"Diferencia volumen: {diferencia_volumen_cm3:.2f} cm¬≥ = {diferencia_volumen_gal:.3f} gal")
                                
                                # Solo guardar si la diferencia de masa supera el volumen m√≠nimo (criterio de validaci√≥n)
                                if diferencia_masa_kg >= vol_minimo_batch:
                                    # Calcular promedios
                                    temperaturas = [d.coriolis_temperature for d in datos_batch if d.coriolis_temperature is not None]
                                    densidades = [d.density for d in datos_batch if d.density is not None]
                                    presiones = [d.pressure_out for d in datos_batch if d.pressure_out is not None]
                                    temp_prom = sum(temperaturas) / len(temperaturas) if temperaturas else 0
                                    dens_prom = sum(densidades) / len(densidades) if densidades else 0
                                    pres_prom = sum(presiones) / len(presiones) if presiones else None
                                    
                                    batches.append({
                                        'fecha_inicio': primer_dato.created_at_iot,
                                        'fecha_fin': fin_batch,
                                        'vol_total': diferencia_volumen_gal,
                                        'mass_total': diferencia_masa_kg,
                                        'temperatura_coriolis_prom': temp_prom,
                                        'densidad_prom': dens_prom,
                                        'pressure_out_prom': pres_prom,
                                        'duracion_minutos': (fin_batch - primer_dato.created_at_iot).total_seconds() / 60,
                                        'total_registros': len(datos_batch),
                                        'vol_minimo_usado': vol_minimo_batch,
                                        'time_finished_usado': time_finished_batch_actual
                                    })
                                    logger.info(f"Batch guardado despu√©s de {minutos_en_cero:.2f} min en cero: {primer_dato.created_at_iot} - {fin_batch}, Vol: {diferencia_volumen_gal:.3f} gal, Masa: {diferencia_masa_kg:.2f} kg")
                                else:
                                    logger.debug(f"Batch descartado: {diferencia_masa_kg:.2f} kg < {vol_minimo_batch} kg")
                            
                            # Reiniciar estado
                            en_batch = False
                            inicio_batch = None
                            primer_dato = None
                            datos_batch = []
                            tiempo_cero_inicio = None
                            ultimo_dato_con_flujo = None
                            # üîÑ Resetear perfil para el siguiente batch
                            vol_minimo_batch = None
                            time_finished_batch_actual = None
                        else:
                            # Si no ha pasado el tiempo, seguir esperando
                            tiempo_restante = time_finished_batch_actual - minutos_en_cero
                            logger.debug(f"‚è≥ Esperando... Faltan {tiempo_restante:.3f} min para cerrar batch")
                else:
                    # No hay batch activo, guardar como punto anterior
                    tiempo_cero_inicio = None
                
                # Guardar este punto como posible referencia para el pr√≥ximo batch
                punto_anterior = dato

        # Si termina con un batch abierto, cerrarlo si cumple el volumen m√≠nimo
        if en_batch and datos_batch and primer_dato is not None and ultimo_dato_con_flujo is not None:
            # C√°lculos de masa
            masa_inicial_lb = primer_dato.total_mass
            masa_final_lb = ultimo_dato_con_flujo.total_mass
            diferencia_masa_lb = masa_final_lb - masa_inicial_lb
            diferencia_masa_kg = lb_a_kg(diferencia_masa_lb)
            
            # C√°lculos de volumen (convertir de cm¬≥ a galones)
            volumen_inicial_cm3 = primer_dato.total_volume
            volumen_final_cm3 = ultimo_dato_con_flujo.total_volume
            diferencia_volumen_cm3 = volumen_final_cm3 - volumen_inicial_cm3
            diferencia_volumen_gal = cm3_a_gal(diferencia_volumen_cm3)
            
            logger.debug(f"Cerrando batch abierto al final")
            logger.debug(f"Masa inicial (punto en 0): {masa_inicial_lb:.2f} lb en {primer_dato.created_at_iot}")
            logger.debug(f"Masa final (√∫ltimo punto >0): {masa_final_lb:.2f} lb en {ultimo_dato_con_flujo.created_at_iot}")
            logger.debug(f"Diferencia masa: {diferencia_masa_lb:.2f} lb = {diferencia_masa_kg:.2f} kg")
            logger.debug(f"Volumen inicial: {volumen_inicial_cm3:.2f} cm¬≥, Volumen final: {volumen_final_cm3:.2f} cm¬≥")
            logger.debug(f"Diferencia volumen: {diferencia_volumen_cm3:.2f} cm¬≥ = {diferencia_volumen_gal:.3f} gal")
            
            if diferencia_masa_kg >= vol_minimo_batch:
                temperaturas = [d.coriolis_temperature for d in datos_batch if d.coriolis_temperature is not None]
                densidades = [d.density for d in datos_batch if d.density is not None]
                presiones = [d.pressure_out for d in datos_batch if d.pressure_out is not None]
                temp_prom = sum(temperaturas) / len(temperaturas) if temperaturas else 0
                dens_prom = sum(densidades) / len(densidades) if densidades else 0
                pres_prom = sum(presiones) / len(presiones) if presiones else None
                
                batches.append({
                    'fecha_inicio': primer_dato.created_at_iot,
                    'fecha_fin': ultimo_dato_con_flujo.created_at_iot,
                    'vol_total': diferencia_volumen_gal,
                    'mass_total': diferencia_masa_kg,
                    'temperatura_coriolis_prom': temp_prom,
                    'densidad_prom': dens_prom,
                    'pressure_out_prom': pres_prom,
                    'duracion_minutos': (ultimo_dato_con_flujo.created_at_iot - primer_dato.created_at_iot).total_seconds() / 60,
                    'total_registros': len(datos_batch),
                    'vol_minimo_usado': vol_minimo_batch,
                    'time_finished_usado': time_finished_batch_actual
                })
                logger.info(f"Batch final guardado: {primer_dato.created_at_iot} - {ultimo_dato_con_flujo.created_at_iot}, Vol: {diferencia_volumen_gal:.3f} gal, Masa: {diferencia_masa_kg:.2f} kg")

        logger.info(f"‚úÖ Detecci√≥n completada con l√≥gica de PERFIL DIN√ÅMICO. {len(batches)} batches detectados")
        return batches
    
    def _generar_hash_batch(self, fecha_inicio, fecha_fin, sistema_id, vol_minimo, time_finished_batch):
        """
        Genera un hash √∫nico basado en las fechas, sistema ID, vol_minimo y time_finished_batch.
        Esto previene duplicados cuando se ejecuta la detecci√≥n m√∫ltiples veces.
        Incluye par√°metros que realmente afectan la detecci√≥n de batches.
        """
        # Crear string √∫nico con par√°metros del batch
        fecha_inicio_str = fecha_inicio.strftime('%Y-%m-%d %H:%M:%S')
        fecha_fin_str = fecha_fin.strftime('%Y-%m-%d %H:%M:%S')
        
        datos_hash = f"{sistema_id}_{fecha_inicio_str}_{fecha_fin_str}_{vol_minimo}_{time_finished_batch}"
        
        # Generar hash SHA-256
        hash_obj = hashlib.sha256(datos_hash.encode('utf-8'))
        return hash_obj.hexdigest()